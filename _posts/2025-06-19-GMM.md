---
layout: post
title: Generalized Method of Moments
tag: study
update: 2025-06-19
---


## Model Setup

$$
\begin{equation}\label{eq-model-gmm}
\begin{split}
y_i &= \bx_i'\bbeta + \varepsilon_i \\
\E[\varepsilon_i \mid z_i] &= 0
\end{split}
\end{equation}
$$

for $K$ variables in $\bx_i$ and for some set of $L$ instrumental variables in $\bz_i,$ where $L\ge K.$


The generalized regression model is a special case where $\bz_i=\bx_i.$

The assumption $\E[\varepsilon_i \mid z_i] = 0$ implies the following **orthogonality condition**:

$$
\cov(\bz_i, \varepsilon_i) = \boldsymbol{0},
$$

or

$$
\E[\bz_i(y_i - \bx_i'\bbeta)] = \boldsymbol{0}.
$$

By summing the terms, we find that this further implies the **population moment equation**,

$$
\begin{equation}\label{eq-pop-ortho-cond}
\E\left[\frac{1}{n} \sum_{i=1}^n \bz_i(y_i - \bx_i'\bbeta) \right] = \E[\bar{\bm}(\bbeta)] = 0
\end{equation}
$$

where 

$$
\bar{\bm}(\bbeta) = \frac{1}{n} \sum_{i=1}^n \bz_i(y_i - \bx_i'\bbeta)
$$

If the population relationship $\eqref{eq-pop-ortho-cond}$ holds for the true parameter vector, $\bbeta,$ suppose we attempt to mimic this result with a sample counterpart, or **empirical moment equation**,

$$
\begin{equation}\label{eq-gmm-empirical-moment}
\left[\frac{1}{n} \sum_{i=1}^n \bz_i(y_i - \bx_i'\hat{\bbeta}) \right] = \left[\frac{1}{n} \sum_{i=1}^n \bm_i(\hat{\bbeta}) \right] = \bar{\bm}(\hat{\bbeta}) = \boldsymbol{0},
\end{equation}
$$

For the identified cases, it is convenient to write $\eqref{eq-gmm-empirical-moment}$ as

$$
\bar{\bm}(\hat{\bbeta}) = \left(\frac{1}{n}\bZ'\by\right) - \left(\frac{1}{n} \bZ'\bX \right)\bbeta = \boldsymbol{0}
$$

which is $L$ equations (the number of variables in $\bZ$) in $K$ unknowns (the number of parameters we seek to estimate).

There are three possibilities to consider:

- Underidentified if $L<K$

    There is no solution as there are fewer moment equations than parameters.

- Exactly identified if $L=K$

    One single solution exists:

    $$
    \hat{\bbeta} = (\bZ'\bX)^{-1} (\bZ\by) ,
    $$

    which is the same as the IV estimator.

- Overidentified if $L>K$

    No unique solution to $\bar{\bm}(\hat{\bbeta}) = \boldsymbol{0}.$ 

    In this instance, we choose the estimator based on the "least squares" criterion:

    $$
    \underset{\bbeta}{\arg\min}\; \bar{\bm}(\hat{\bbeta})'\bar{\bm}(\hat{\bbeta}) .
    $$


## GMM Estimator

Let $q=\bar{\bm}(\hat{\bbeta})'\bar{\bm}(\hat{\bbeta})$, the first order conditions are

$$
\begin{split}
\frac{\partial q}{\partial \bbeta} 
&= 2 \left(\frac{\partial \bar{\bm}(\hat{\bbeta})'}{\partial \bbeta}\right) \bar{\bm}(\hat{\bbeta}) \\
&= 2 \bar{\bG}(\hat{\bbeta})' \bar{\bm}(\hat{\bbeta}) \\
&= 2\left(\frac{1}{n}\bX'\bZ\right) \left(\frac{1}{n}\bZ'\by - \frac{1}{n}\bZ'\bX\hat{\bbeta} \right) \\
&= \boldsymbol{0} .
\end{split}
$$

Then we have

$$
\hat{\bbeta} = [(\bX'\bZ)(\bZ'\bX)]^{-1} (\bX'\bZ)(\bZ'\by) .
$$