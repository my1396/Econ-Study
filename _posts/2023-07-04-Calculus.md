---
layout: post
title: Calculus
tag: study
modified_date: 2023-10-21
---
<a class="top-link" href="#" id="js-top">â†‘</a>

## **Trig identities**

$\mathrm{sin}\theta$ å¥‡å‡½æ•°

$\mathrm{cos}\theta$ å¶å‡½æ•°

$\sec^2=1+\tan^2$

$\mathrm{sin}(\theta\pm\frac{k\pi}{2})$,  $\mathrm{cos}(\theta\pm\frac{k\pi}{2})$ å¥‡å˜å¶ä¸å˜ï¼Œç¬¦å·çœ‹è±¡é™ã€‚ 

-   æ ¹æ®$k$ ï¼Œå†³å®šæ˜¯å¦å˜å‡½æ•°ã€‚ $k$ ä¸ºå¥‡æ•°ï¼Œ$\mathrm{sin}$ å˜ $\mathrm{cos}$ï¼Œ$\mathrm{cos}$ å˜ $\mathrm{sin}$ã€‚$k$ ä¸ºå¶æ•°ï¼Œåˆ™å‡½æ•°åä¸å˜ã€‚
-   ç¬¦å·ç”±åŸå‡½æ•°å†³å®š, assuming $0<\theta<\frac{\pi}{2}$ä¸ºé”è§’ï¼Œçœ‹$\theta\pm\frac{k\pi}{2}$åœ¨åŸå‡½æ•°çš„æ­£è´Ÿã€‚å®é™…ä¸Š $\theta$ å¯ä»¥æ˜¯ä»»æ„è§’åº¦ï¼Œä½†å¦‚æœæ˜¯é’è§’ï¼Œå®Œå…¨å¯ä»¥$k:=k+1$åŒ–ä¸ºé”è§’ï¼›å¦åˆ™ï¼Œå‡½æ•°å¤„äºæœªåŒ–ç®€å®Œå…¨çš„çŠ¶æ€ã€‚
-   e.g., $\mathrm{cos}(\frac{\pi}{2}-\theta)=\mathrm{sin}\theta$, $k=1$ä¸ºå¥‡æ•°, å‡½æ•°åå˜ä¸º$\mathrm{sin}$; å½“$\theta$ä¸ºé”è§’æ—¶ï¼Œ$\frac{\pi}{2}-\theta$ åœ¨ç¬¬ä¸€è±¡é™ï¼Œ$\mathrm{cos}(\frac{\pi}{2}-\theta)$ä¸ºæ­£ã€‚ 



double-angel formula å€è§’å…¬å¼

-   $\mathrm{cos}(2\theta)=\mathrm{cos}^2\theta-\mathrm{sin}^2\theta$

-   $\mathrm{sin}(2\theta)=2\mathrm{sin}\theta\cdot \mathrm{cos}\theta$, 2 ä¸è¦è½ä¸‹äº†ï¼

half-angel formula åŠè§’å…¬å¼ (é™å¹‚å…¬å¼) used to get rid of the power

-   $\mathrm{cos}^2\theta=\frac{1+\mathrm{cos}(2\theta)}{2}$

-   $\mathrm{sin}^2\theta=\frac{1-\mathrm{cos}(2\theta)}{2}$


**å’Œå·®å…¬å¼** Angle sum and difference identities

$\mathrm{sin}(\alpha+\beta)=\mathrm{sin}\alpha\cdot\mathrm{cos}\beta+\mathrm{cos}\alpha\cdot\mathrm{sin}\beta$

$\mathrm{sin}(\alpha-\beta)=\mathrm{sin}\alpha\cdot\mathrm{cos}\beta-\mathrm{cos}\alpha\cdot\mathrm{sin}\beta$

$\mathrm{cos}(\alpha+\beta)=\mathrm{cos}\alpha\cdot\mathrm{cos}\beta-\mathrm{sin}\alpha\cdot\mathrm{sin}\beta$

$\mathrm{cos}(\alpha-\beta)=\mathrm{cos}\alpha\cdot\mathrm{cos}\beta+\mathrm{sin}\alpha\cdot\mathrm{sin}\beta$



**ç§¯åŒ–å’Œå·®**å…¬å¼å¯ä»¥ç”±å’Œå·®å…¬å¼å¾—æ¥ã€‚(product-to-sum identities)



**å’Œå·®åŒ–ç§¯**å…¬å¼ (sum-to-product identities)

æ­£åŠ æ­£ï¼Œæ­£åœ¨å‰ï¼› $\mathrm{sin}\alpha +\mathrm{sin}\beta=2\, \mathrm{sin}\frac{\alpha+\beta}{2}\cdot\mathrm{cos}\frac{\alpha-\beta}{2}$

æ­£å‡ä½™åœ¨å…ˆï¼› $\mathrm{sin}\alpha - \mathrm{sin}\beta=2\, \mathrm{cos}\frac{\alpha+\beta}{2}\cdot\mathrm{sin}\frac{\alpha-\beta}{2}$

ä½™åŠ å…¨æ˜¯ä½™ï¼›$\mathrm{cos}\alpha +\mathrm{cos}\beta=2\, \mathrm{cos}\frac{\alpha+\beta}{2}\cdot\mathrm{cos}\frac{\alpha-\beta}{2}$

ä½™å‡è´Ÿæ­£å¼¦ï¼›$\mathrm{cos}\alpha - \mathrm{cos}\beta=-2\, \mathrm{sin}\frac{\alpha+\beta}{2}\cdot\mathrm{sin}\frac{\alpha-\beta}{2}$



## Calculus

1st derivative

$y'(x)=0$ max/min point, change of directions (from up to down if max, from down to up if min).

2nd derivative

$y^{\prime\prime}=0$ inflection point, change of concavity.



Convex function $\cup$

- The line segment between any two distinct points lies *above* the graph between the two points.
- 2nd derivative $f''$ is nonnegative, i.e., $f'$ is monotonically increasing.

Concave function $\cap$

- The line segment between any two distinct points lies *below* the graph between the two points.
- 2nd derivative $f''$ is nonpositive, i.e., $f'$ is monotonically decreasing.



How to find **min/max** pt.

-   look at critical points (where $f'=0$), end points, and discontinuities. Critical values are the values of the function where $x$ equal to critical points.

    -   1st derivative rule, then you get critical points. Have to check whether it is min or max. Two ways of doing it

        1.   calculate end points and compare to critical points;
        2.   calculate $f^{\prime\prime}$, if $f^{\prime\prime}<0$ then max; if $f^{\prime\prime}>0$ then min; (2nd derivate tell how 1st derivative changes. If at min, the func will increase, i.e., $f'$ $\uparrow$; if at max, the func will decrease, i.e., $f' \downarrow$. )

        -   Note: it is NOT recomended to calculate the 2nd derivative b/c usually it is easier to calculate the end points. 2nd derivatives are usually difficult to find if you have a complicated func.

-   max/min point corresponds to critical points; alternatively, where is max/min achieved?

-   max/min value corresponds to critical value;



**Newton's Method**
$$
\begin{align*}
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\end{align*}
$$
Newton's method works well if $i$) $\vert f'\vert$ is not too small;  $ii$) $\vert f^{\prime\prime}\vert$ is not too big; and $iii$) $x_0$ is nearby $x$.

Newton's method fails: $i$) find an unexpected root (depending on where the initial guess is); $ii$) completely fail. It repeats in a cycle, and never converges to a single value.

<img src="https://drive.google.com/uc?id=1B7LjCAsBGxIGPKAHy-ogyZtuxjr-Qf5D" alt="Fail of Newton method illustration" style="display: block; margin-right: auto; margin-left: auto; zoom:30%;" />





**MVT vs. IVT**

MVT says let $f:[a,b]\rightarrow \mathbb{R}$ be a continuous function on the closed interval $[a,b]$, and differentiable on the open interval $(a,b)$. Then there exists some $c$ in $(a,b)$ such that 
$$
\begin{align*}
f'(c)=\frac{f(b)-f(a)}{b-a}
\end{align*}
$$
<img src="https://drive.google.com/uc?id=1_-hQu9XVysS4kkZ1LdsVA1qv_-eYsFW-" alt="MVT.png" style="display: block; margin-right: auto; margin-left: auto; zoom:80%;" />



IVT says let $f:[a,b]\rightarrow \mathbb{R}$ be a continuous function on the closed interval $[a,b]$, $\textrm{w}$ is a number between $f(a)$ and $f(b)$, then there must be at least one value $c\in [a,b]$ such that $f(c)=\textrm{w}$.

<img src="https://github.com/my1396/Econ-Study/assets/14339495/e84211f5-038e-41d3-bf78-40785de291d6" alt="IVT.svg" style="display: block; margin-right: auto; margin-left: auto; zoom:150%;" />



-   The **mean value theorem** is all about the differentiable functions and derivatives, whereas the **intermediate value theorem** is about the continuous function. 
-   The MVT guarantees that the derivatives have certain values, whereas the IVT  guarantees that the function has certain values between two given values.



Anti-differentiation used interchangeablely with Integration.

Anti-derivatives $\Longleftrightarrow$ Indefinite Integrals



### **Integral formulas**

Trigonometric integrals

1.   $\int \frac{1}{\sqrt{1-x^2}} dx=\mathrm{sin}^{-1}x+c$, note that $\mathrm{sin}^{-1}x$ denotes "inverse sin" or $\mathrm{arcsin}(x)$, not $\frac{1}{\mathrm{sin}x}$.
2.   $\int -\frac{1}{\sqrt{1-x^2}} dx=\mathrm{cos}^{-1}x+c$
3.   $\int \frac{1}{1+x^2} dx=\mathrm{tan}^{-1}x+c$
4.   $(\tan x)'=\sec^2x$, or $\int \mathrm{sec}^2x\,dx=\mathrm{tan}x+c$.
5.   $\sec(x)'=\sec x\tan x$, or $\int \sec x\tan x\, dx=\sec x +c$.
6.   $\int \mathrm{tan}x\,dx=-\ln(\cos x)+c$.
7.   $\int \sec x\,dx=\ln(\sec x+\tan x)+c$, substitution $u=\sec x+\tan x$.



Common integrals/ derivatives

$\int\frac{1}{y}dy=\ln\vert y\vert +c$

$\sqrt{x}=\frac{1}{2\sqrt{x}}$




Inverse function

Let $y=f(x)$, then $f^{-1}(y)=x$, define $g(y)=f^{-1}(y)$.

Rewrite $x=g(y)$.
Take derivative w.r.t $x$ on both sides,

$$
\begin{align*}
\frac{d}{dx} x &= \frac{d}{dx} g(y) \\
1 &= \frac{dg(y)}{dy} \cdot \frac{dy}{dx} \quad \text{(by chain rule)} \\
1 &= g'(y)\cdot f'(x) \\
g'(y) &= \frac{1}{f'(x)}
\end{align*}
$$

That is $[f^{-1}]'(y) = \frac{1}{f'(x)}$. Derivative of $f^{-1}$ equals to the reciprocal of the derivative of $f$, evaluated at the value of the inverse function ($x=f^{-1}(y)$).


**Jacobian Matrix**

Suppose $f$: $R_n \rightarrow R_m$ is a function such that each of its first-order partial derivatives exist on $R_n$. \
Then the Jacobian matrix of f is defined to be an mÃ—n matrix, denoted by J, whose $(i,j)$ th entry is $\mathbf{J} _{ij}={\frac{\partial f_{i}}{\partial x_{j}}}$. \



**FTC1**

Let $f$ be continuous on $[a,b]$ and let $F$ be any antiderivative of $f$. Then
$$
\begin{align*}
\int_a^bf(t)dt=F(b)-F(a)
\end{align*}
$$


**FTC2**

Let $f$ be continuous on $[a,b]$ and let $F(x)=\int_a^xf(t)dt$. Then $F$ is a differentiable function on $(a,b)$, and 
$$
\begin{align*}
F'(x)=\frac{d}{dx}\int_a^x f(t)dt=f(x).
\end{align*}
$$

-   Important to differentiate variable of integrand $t$ from the upper limit $x$. Use different letters!
-   The derivative of an integration is just replacing the integrand variable with the upper limit variable.



**Trig integrals and Substitution**

To integrate  $\int \cos^jx \sin^kx\,dx$  use the following strategies:

1. If  ğ‘˜  is odd, rewrite  sin$^k$ğ‘¥=sin$^{ğ‘˜âˆ’1}$ğ‘¥sinğ‘¥  and use the identity  sin$^2$ğ‘¥=1âˆ’cos$^2$ğ‘¥  to rewrite  sin$^{ğ‘˜âˆ’1}$ğ‘¥  in terms of  cosğ‘¥ . Integrate using the substitution  ğ‘¢=cosğ‘¥ . This substitution makes  ğ‘‘ğ‘¢=âˆ’sinğ‘¥ğ‘‘ğ‘¥. 

2. If  ğ‘—  is odd, rewrite  cos$^ğ‘—$ğ‘¥=cos$^{ğ‘—âˆ’1}$ğ‘¥cosğ‘¥  and use the identity  cos$^2$ğ‘¥=1âˆ’sin$^2$ğ‘¥  to rewrite  $\mathrm{cos}^{ğ‘—âˆ’1}x$  in terms of  sinğ‘¥ . Integrate using the substitution  ğ‘¢=sinğ‘¥ . This substitution makes  ğ‘‘ğ‘¢=cosğ‘¥ğ‘‘ğ‘¥.  (Note: If both  ğ‘—  and  ğ‘˜  are odd, either strategy 1 or strategy 2 may be used.)

3. If both  ğ‘—  and  ğ‘˜  are even, use  sin$^2$ğ‘¥=$\frac{1âˆ’\cos(2ğ‘¥)}{2}$  and  cos$^2$ğ‘¥=$\frac{1+\cos(2ğ‘¥)}{2}$ . After applying these formulas, simplify and reapply strategies 1 through 3 as appropriate.

>   Summary: 
>
>   1.   integrand only includes powers of $\mathrm{sin}x$ and $\mathrm{cos}x$.
>
>   2.   find the odd power and separate 1 as derivative of the substitution. Rewrite the integrand as a function of the other trig.
>   3.   if only even exponents are present, use half-angel fornula to lower the power (double the trig angel) until you have an odd exponent.



Integrating âˆ«tan$^ğ‘˜$ğ‘¥sec$^ğ‘—$ğ‘¥ğ‘‘ğ‘¥

1.   If ğ‘— is even and ğ‘—â‰¥2, rewrite sec$^ğ‘—$ğ‘¥=sec$^{ğ‘—âˆ’2}$ğ‘¥sec$^2$ğ‘¥ and use sec$^2$ğ‘¥=tan$^2$ğ‘¥+1 to rewrite sec$^{ğ‘—âˆ’2}$ğ‘¥ in terms of tanğ‘¥. Let ğ‘¢=tanğ‘¥ and ğ‘‘ğ‘¢=sec$^2$ğ‘¥.

2.   If ğ‘˜ is odd and ğ‘—â‰¥1, rewrite tan$^ğ‘˜$ğ‘¥sec$^ğ‘—$ğ‘¥=tan$^{ğ‘˜âˆ’1}$ğ‘¥sec$^{ğ‘—âˆ’1}$ğ‘¥secğ‘¥tanğ‘¥ and use tan$^2$ğ‘¥=sec$^2$ğ‘¥âˆ’1 to rewrite tan$^{ğ‘˜âˆ’1}$ğ‘¥ in terms of secğ‘¥. Let ğ‘¢=secğ‘¥ and ğ‘‘ğ‘¢=secğ‘¥tanğ‘¥ğ‘‘ğ‘¥. (Note: If ğ‘— is even and ğ‘˜ is odd, then either strategy 1 or strategy 2 may be used.)

3.   If ğ‘˜ is odd where ğ‘˜â‰¥3 and ğ‘—=0, rewrite tan$^ğ‘˜$ğ‘¥=tan$^{ğ‘˜âˆ’2}$ğ‘¥tan$^2$ğ‘¥=tan$^{ğ‘˜âˆ’2}$ğ‘¥(sec$^2$ğ‘¥âˆ’1)=tan$^{ğ‘˜âˆ’2}$ğ‘¥sec$^2$ğ‘¥âˆ’tan$^{ğ‘˜âˆ’2}$ğ‘¥. It may be necessary to repeat this process on the tan$^{ğ‘˜âˆ’2}$ğ‘¥ term.

4.   If ğ‘˜ is even and ğ‘— is odd, then use tan$^2$ğ‘¥=sec$^2$ğ‘¥âˆ’1 to express tan$^ğ‘˜$ğ‘¥ in terms of secğ‘¥. Use integration by parts to integrate odd powers of secğ‘¥.



**Reduction formulas**

1.   $\int \sec^n xdx=\frac{1}{n-1}\sec^{n-2}x\tan x+\frac{n-2}{n-1}\int \sec^{n-2}xdx$, $n$â€‹ is odd. Verified by applying integration by parts.
2.   $\int \tan ^n xdx=\frac{1}{n-1}\tan ^{n-1}x-\int\tan^{n-2}x dx$, $n$ is odd.



**Method of Substitution**

1.   The original integrand (in $g(x)dx$ or $h(t)dt$ ... ) can be expressed as $f(u)\,du$.
2.   Usually $g(x)$ and $h(t)$ coule be a mess, but it is much easier to find the anti-derivative of $f(u)$.

**Families of Densities**

Standard Normal: $Z \sim N(0,1)$

$$
\phi(z) = \frac{1}{\sqrt(2\pi)} e^{-\frac{x^2}{2}}
$$

Normal distribution $X \sim N(\mu,\sigma^2)$

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

**Jacobian and Jacobian Matrix**

Jacobian matrix is a matrix of partial derivatives. Jacobian is the **determinant** of the jacobian matrix. The matrix will contain all partial derivatives of a vector function. The main use of Jacobian is found in the transformation of coordinates. It deals with the concept of differentiation with coordinate transformation.

å‡è®¾$F:R_nâ†’R_m$ æ˜¯ä¸€ä¸ªä»æ¬§å¼ $n$ ç»´ç©ºé—´è½¬æ¢åˆ°æ¬§å¼ $m$ ç»´ç©ºé—´çš„å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°ç”±$m$ä¸ªå®å‡½æ•°ç»„æˆï¼š$ y_1=f_1(x_1, \ldots, x_n), \ldots, y_m=f_m(x_1,\ldots, x_n)$. \
è¿™äº›å‡½æ•°çš„åå¯¼æ•°(å¦‚æœå­˜åœ¨)å¯ä»¥ç»„æˆä¸€ä¸ª $m$ (dimension of $y$) è¡Œ $n$ (dimension of $x$) åˆ—çš„çŸ©é˜µï¼Œè¿™å°±æ˜¯æ‰€è°“çš„é›…å¯æ¯”çŸ©é˜µ

$$
\mathbf{J} = \begin{bmatrix} 
\frac{\partial \mathbf{y}}{\partial x_1} & \cdots & \frac{\partial \mathbf{y}}{\partial x_n}
\end{bmatrix} =
\begin{bmatrix}
\nabla^Tf_1 \\
\vdots \\
\nabla^Tf_m
\end{bmatrix} = 
\begin{bmatrix} 
\frac{\partial y_1}{\partial x_1} &  \cdots &  \frac{\partial y_1}{\partial x_m} \\
 \vdots &  \ddots & \vdots  \\
\frac{\partial y_n}{\partial x_1} &  \cdots & \frac{\partial y_n}{\partial x_m} \end{bmatrix}
$$

$\nabla^Tf_i$ the transpose (row vector) of the gradient of the $i$-th component of $f$.
The Jacobian matrix can be also written as

$$
\begin{bmatrix} 
\frac{\partial f_1}{\partial x_1} &  \cdots &  \frac{\partial f_1}{\partial x_m} \\
 \vdots &  \ddots & \vdots  \\
\frac{\partial f_n}{\partial x_1} &  \cdots & \frac{\partial f_n}{\partial x_m} \end{bmatrix}
$$

æ­¤çŸ©é˜µè¡¨ç¤ºä¸ºï¼š$J_F(x_1,\cdots ,x_2)$ï¼Œæˆ–è€…: $\frac{\partial (y_1,\ldots,y_n)}{\partial (x_1, \ldots ,x_n)}$ or $\frac{\partial (f_1,\ldots,f_n)}{\partial (x_1, \ldots ,x_n)}$
- The $i$-th row contains the derivative of the $i$-th components of $y_i$ with respect to all variables, denoted $\frac{\partial f_i}{\partial x}$.
- The $k$-th column contains the all $m$ components of $f$ with respect to the $k$-th variable, denoted $\frac{\partial f}{\partial x_k}$.

å½“ $m=n$ æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡æœ‰6ä¸ªå‡½æ•°ï¼Œæ¯ä¸ªå‡½æ•°å¯¹åº”ç€æœ‰6ä¸ªå˜é‡ã€‚é‚£ä¹ˆé’ˆå¯¹æ¯ä¸ªè¾“å…¥å˜é‡ $x_i$ï¼Œå°±ä¼šèƒ½å¤Ÿå¾—åˆ°å¯¹åº”çš„ $y_i$

$$
\begin{align*}
y_1 &= f_1(x_1,x_2,x_3,x_4,x_5,x_6) \\
y_2 &= f_2(x_1,x_2,x_3,x_4,x_5,x_6) \\
y_3 &= f_3(x_1,x_2,x_3,x_4,x_5,x_6) \\
y_4 &= f_4(x_1,x_2,x_3,x_4,x_5,x_6) \\
y_5 &= f_5(x_1,x_2,x_3,x_4,x_5,x_6) \\
y_6 &= f_6(x_1,x_2,x_3,x_4,x_5,x_6)
\end{align*}
$$

å› æ­¤ $y_i$ çš„å¯¼æ•°å¯ä»¥è¢«å†™æˆ

$$
\mathrm{d} {y_i} = \frac{\partial f_i}{\partial x_1}{\mathrm{d} x_1} + \frac{\partial f_i}{\partial x_2}{\mathrm{d} x_2}+\frac{\partial f_i}{\partial x_3}{\mathrm{d} x_3}+\frac{\partial f_i}{\partial x_4}{\mathrm{d} x_4}+\frac{\partial f_i}{\partial x_5}{\mathrm{d} x_5}+\frac{\partial f_i}{\partial x_6}{\mathrm{d} x_6}
$$

å› æ­¤ç»“åˆä¸Šé¢çš„æ–¹ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢çš„æ–¹ç¨‹å†™ä¸ºå‘é‡çš„å½¢å¼ï¼š

$$
{\mathrm{d} Y} = \frac{\partial F}{\partial X}{\mathrm{d} X}
$$

å‡½æ•°Få¯¹äºXçš„åå¯¼æ•°çŸ©é˜µï¼Œå°±è¢«ç§°ä¸ºé›…å¯æ¯”çŸ©é˜µ(Jacobian Matrix)ã€‚

*Jacobian* is the determinant of the matrix.



Reference: <https://loopvoid.github.io/2018/04/28/Jacobian%E7%9F%A9%E9%98%B5%E4%B8%8EHessian%E7%9F%A9%E9%98%B5%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98/>